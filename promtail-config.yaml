server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  # Docker container logs
  - job_name: containers
    static_configs:
      - targets:
          - localhost
        labels:
          job: dockerlogs
          __path__: /var/lib/docker/containers/*/*log

    pipeline_stages:
      # Extract container info from log
      - json:
          expressions:
            output: log
            stream: stream
            attrs:
      
      # Extract container labels
      - json:
          expressions:
            tag:
          source: attrs
      
      # Parse container name from Docker tag
      - regex:
          expression: '(?P<container_name>(?:[^|]*))\|(?P<image_name>(?:[^|]*))'
          source: tag
      
      # Extract DocAIche service name from container name
      - regex:
          expression: '^/?docaiche-(?P<service>[^-]+)'
          source: container_name
      
      # Fallback: extract generic service name  
      - regex:
          expression: '^/?(?P<service_name>[^-]+)'
          source: container_name
      
      # Add timestamp
      - timestamp:
          format: RFC3339Nano
          source: time
      
      # Set labels for Loki
      - labels:
          stream:
          service:
          service_name:
          container_name:
      
      # Parse log levels if present
      - regex:
          expression: '(?i)(?P<level>(DEBUG|INFO|WARN|WARNING|ERROR|FATAL))'
      
      - labels:
          level:
      
      # Parse AI-specific fields from JSON logs
      - json:
          expressions:
            correlation_id: correlation_id
            conversation_id: conversation_id
            workspace_id: workspace_id
            model: model
            tokens_used: tokens_used
            request_type: request_type
            user_id: user_id
            session_id: session_id
            duration: duration
          source: output
      
      # Add AI-specific labels
      - labels:
          correlation_id:
          conversation_id:
          workspace_id:
          model:
          request_type:
      
      # Extract and track AI token usage as metric (only if fields exist)
      # Commented out metrics to fix negative metadata errors
      # - match:
      #     selector: '{tokens_used!=""}'
      #     stages:
      #       - metrics:
      #           ai_tokens_used:
      #             type: Counter
      #             description: "Total AI tokens used"
      #             source: tokens_used
      #             config:
      #               action: add
      
      # - match:
      #     selector: '{duration!=""}'
      #     stages:
      #       - metrics:
      #           ai_request_duration:
      #             type: Histogram
      #             description: "AI request duration in seconds"
      #             source: duration
      #             config:
      #               buckets: [0.1, 0.5, 1, 2, 5, 10, 30]
      
      # Output the actual log message
      - output:
          source: output

  # DocAIche API logs
  - job_name: docaiche_api
    static_configs:
      - targets:
          - localhost
        labels:
          job: docaiche
          service: api
          __path__: /var/log/docaiche/api.log

  # Admin UI logs  
  - job_name: docaiche_admin
    static_configs:
      - targets:
          - localhost
        labels:
          job: docaiche
          service: admin-ui
          __path__: /var/log/docaiche/admin.log

  # AI Agent logs  
  - job_name: ai_agent_logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: docaiche
          service: ai_agent
          __path__: /var/log/docaiche/ai_*.log
    
    pipeline_stages:
      # Parse JSON logs
      - json:
          expressions:
            timestamp: timestamp
            level: level
            message: message
            correlation_id: correlation_id
            conversation_id: conversation_id
            workspace_id: workspace_id
            model: model
            tokens_used: tokens_used
            duration: duration
            request_type: request_type
            user_id: user_id
            session_id: session_id
            error: error
            prompt_hash: prompt_hash
      
      # Use extracted timestamp
      - timestamp:
          format: RFC3339Nano
          source: timestamp
      
      # Set labels
      - labels:
          level:
          correlation_id:
          conversation_id:
          workspace_id:
          model:
          request_type:
      
      # Track metrics (only if fields exist)
      # Commented out metrics to fix negative metadata errors
      # - match:
      #     selector: '{tokens_used!=""}'
      #     stages:
      #       - metrics:
      #           ai_tokens_total:
      #             type: Counter
      #             description: "Total AI tokens consumed"
      #             source: tokens_used
      #             config:
      #               action: add
      
      # - match:
      #     selector: '{duration!=""}'
      #     stages:
      #       - metrics:
      #           ai_request_duration_seconds:
      #             type: Histogram
      #             description: "AI request duration"
      #             source: duration
      #             config:
      #               buckets: [0.1, 0.5, 1, 2, 5, 10, 30]
      
      # Output message
      - output:
          source: message

  # System logs
  - job_name: syslog
    journal:
      max_age: 12h
      labels:
        job: systemd-journal
    relabel_configs:
      - source_labels: ['__journal__systemd_unit']
        target_label: 'unit'