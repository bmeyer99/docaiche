services:
  api:
    build:
      context: .
      dockerfile: src/api/Dockerfile
    # Port not exposed - access through admin-ui proxy on port 4080
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      anythingllm:
        condition: service_healthy
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - ANYTHINGLLM_URL=http://anythingllm:3001
      - ANYTHINGLLM_API_KEY=docaiche-lab-default-key-2025
      - DATABASE_URL=sqlite+aiosqlite:///data/docaiche.db
      - DB_PATH=/data/docaiche.db
      - ENVIRONMENT=development
      - DATA_DIR=/data
      - LOG_LEVEL=INFO
      # AI Logging Configuration
      - AI_LOG_ENABLED=true
      - AI_LOG_CACHE_TTL=300
      - AI_LOG_MAX_CACHE_SIZE=1000
      - AI_LOG_PATTERN_DETECTION=true
      - AI_LOG_CORRELATION_TIMEOUT=30
      - AI_LOG_STREAM_BUFFER_SIZE=100
      - AI_LOG_EXPORT_BATCH_SIZE=1000
      - LOKI_URL=http://loki:3100
      - LOKI_TIMEOUT=30
      - LOKI_MAX_RETRIES=3
    command: >
      sh -c "
        mkdir -p /var/log/docaiche &&
        alembic upgrade head &&
        python src/database/init_db.py --db-path /data/docaiche.db &&
        (python /app/init-anythingllm-workspace.py > /var/log/docaiche/workspace-init.log 2>&1 &) &&
        uvicorn src.main:app --host 0.0.0.0 --port 4000
      "
    networks:
      - docaiche
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/api/v1/health"]
      interval: 5s
      timeout: 4s
      retries: 10
    volumes:
      - db_data:/data
      - ./config.yaml:/app/config.yaml:ro
      - logs_data:/var/log/docaiche
    restart: unless-stopped
    labels:
      - "docaiche.service=api"

  admin-ui:
    build:
      context: ./admin-ui
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_APP_NAME=DocAIche Admin
        - NEXT_PUBLIC_ENABLE_AUTH=false
    ports:
      - "4080:3000"
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_APP_NAME=DocAIche Admin
      - NEXT_PUBLIC_ENABLE_AUTH=false
      - NEXT_PUBLIC_SENTRY_DISABLED=true
      # AI Logging Access for UI
      - NEXT_PUBLIC_AI_LOGS_ENABLED=true
      - NEXT_PUBLIC_AI_LOGS_WEBSOCKET_ENABLED=true
    depends_on:
      api:
        condition: service_healthy
    networks:
      - docaiche
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000/api/health', (res) => process.exit(res.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    volumes:
      - logs_data:/var/log/docaiche
    restart: unless-stopped
    labels:
      - "docaiche.service=admin-ui"

  redis:
    image: redis:7-alpine
    command: ["redis-server", "--save", "60", "1", "--loglevel", "warning"]
    volumes:
      - redis_data:/data
    networks:
      - docaiche
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 4s
      retries: 10
    restart: unless-stopped
    labels:
      - "docaiche.service=redis"

  db:
    image: alpine:3.18
    command: ["sh", "-c", "mkdir -p /data && touch /data/docaiche.db && chmod 666 /data/docaiche.db && tail -f /dev/null"]
    volumes:
      - db_data:/data
    networks:
      - docaiche
    healthcheck:
      test: ["CMD", "test", "-f", "/data/docaiche.db"]
      interval: 5s
      timeout: 4s
      retries: 10
    restart: unless-stopped
    labels:
      - "docaiche.service=database"

  anythingllm:
    image: mintplexlabs/anythingllm:latest
    environment:
      STORAGE_DIR: /app/server/storage
      SERVER_PORT: 3001
      JWT_SECRET: docaiche-lab-jwt-secret-2025
      AUTH_TOKEN: docaiche-lab-default-key-2025
      DISABLE_TELEMETRY: "true"
    volumes:
      - anythingllm_data:/app/server/storage
      - anythingllm_exports:/app/server/exports
    networks:
      - docaiche
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/v1/health"]
      interval: 5s
      timeout: 4s
      retries: 10
    restart: unless-stopped
    labels:
      - "docaiche.service=anythingllm"

  loki:
    image: grafana/loki:3.5.0
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./loki-config.yaml:/etc/loki/local-config.yaml
      - loki_data:/loki
    networks:
      - docaiche
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    labels:
      - "docaiche.service=loki"

  promtail:
    image: grafana/promtail:3.5.0
    volumes:
      - ./promtail-config.yaml:/etc/promtail/config.yml
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
      - logs_data:/var/log/docaiche:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      - docaiche
    depends_on:
      - loki
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9080/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    labels:
      - "docaiche.service=promtail"

  grafana:
    image: grafana/grafana:12.0.2
    ports:
      - "3001:3000"  # Different port to avoid conflict with admin-ui
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_ANALYTICS_CHECK_FOR_PLUGIN_UPDATES=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    networks:
      - docaiche
    depends_on:
      - loki
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    labels:
      - "docaiche.service=grafana"

  prometheus:
    image: prom/prometheus:v3.4.2
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    networks:
      - docaiche
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    labels:
      - "docaiche.service=prometheus"

  node-exporter:
    image: prom/node-exporter:v1.8.2
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      - docaiche
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9100/metrics || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    labels:
      - "docaiche.service=node-exporter"

  redis-exporter:
    image: oliver006/redis_exporter:v1.66.0
    environment:
      - REDIS_ADDR=redis://redis:6379
    networks:
      - docaiche
    depends_on:
      - redis
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9121/metrics || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    labels:
      - "docaiche.service=redis-exporter"

networks:
  docaiche:

volumes:
  redis_data:
  db_data:
  anythingllm_data:
  anythingllm_exports:
  loki_data:
  grafana_data:
  prometheus_data:
  logs_data: