# AI Documentation Cache System Configuration
# PRD-003 CFG-008: Default Configuration File
# Comprehensive production defaults for all configuration sections

# Application Configuration
app:
  version: "1.0.0"
  environment: "production"  # development, production, testing
  debug: false
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  data_dir: "/app/data"
  api_host: "0.0.0.0"
  api_port: 8080
  web_port: 8081
  workers: 4

# Content Processing Configuration
content:
  chunk_size_default: 1000
  chunk_size_max: 4000
  chunk_overlap: 100
  quality_threshold: 0.3
  min_content_length: 50
  max_content_length: 1000000

# Weaviate Vector Database Configuration
weaviate:
  endpoint: "http://weaviate:8080"
  api_key: "${WEAVIATE_API_KEY}"  # Set via environment variable
  circuit_breaker:
    failure_threshold: 3
    recovery_timeout: 60
    timeout_seconds: 30

# GitHub Integration Configuration
github:
  api_token: "${GITHUB_API_TOKEN}"  # Set via environment variable
  circuit_breaker:
    failure_threshold: 5
    recovery_timeout: 300
    timeout_seconds: 30

# Web Scraping Configuration
scraping:
  user_agent: "DocAIcheBot/1.0"
  rate_limit_delay: 1.0
  circuit_breaker:
    failure_threshold: 3
    recovery_timeout: 120
    timeout_seconds: 15

# Redis Cache Configuration
redis:
  host: "redis"
  port: 6379
  password: null  # Set REDIS_PASSWORD environment variable for production
  db: 0
  
  # Connection pool settings
  max_connections: 20
  connection_timeout: 5
  socket_timeout: 5
  
  # Memory and persistence settings
  # Note: These must align with docker-compose.yml Redis service configuration
  maxmemory: "512mb"
  maxmemory_policy: "allkeys-lru"
  appendonly: true
  
  # Security settings
  ssl: false
  ssl_cert_reqs: null

# AI Provider Configuration
ai:
  primary_provider: "ollama"
  fallback_provider: "openai"
  enable_failover: true
  cache_ttl_seconds: 3600
  
  # Ollama Configuration
  ollama:
    endpoint: "http://192.168.4.204:11434"
    model: "phi4:14b"
    temperature: 0.7
    max_tokens: 4096
    timeout_seconds: 60
    circuit_breaker:
      failure_threshold: 3
      recovery_timeout: 60
      timeout_seconds: 30
  
  # OpenAI Configuration
  openai:
    api_key: "${OPENAI_API_KEY}"  # Set via environment variable
    model: "gpt-3.5-turbo"
    temperature: 0.7
    max_tokens: 4096
    timeout_seconds: 30
    circuit_breaker:
      failure_threshold: 5
      recovery_timeout: 300
      timeout_seconds: 30

# Knowledge Enrichment Configuration
enrichment:
  max_concurrent_tasks: 5
  task_timeout_seconds: 300
  retry_delay_seconds: 60
  queue_poll_interval: 10
  batch_size: 10
  enable_relationship_mapping: true
  enable_tag_generation: true
  enable_quality_assessment: true
  min_confidence_threshold: 0.7
  sync_ingestion: true  # Enable synchronous ingestion for Context7 results
  sync_ingestion_timeout: 10  # Timeout in seconds for sync ingestion

# MCP External Search Providers Configuration
mcp:
  external_search:
    enabled: true
    providers:
      brave_search:
        enabled: true
        api_key: "${BRAVE_API_KEY}"  # Get free API key from https://brave.com/search/api/
        priority: 1
        max_requests_per_minute: 60
        timeout_seconds: 3
      google_search:
        enabled: false  # Enable when API key available
        api_key: "${GOOGLE_SEARCH_API_KEY}"
        search_engine_id: "${GOOGLE_SEARCH_ENGINE_ID}"
        priority: 2
        max_requests_per_minute: 100
        timeout_seconds: 3
      duckduckgo:
        enabled: false  # Disabled - no official API available
        priority: 3
        max_requests_per_minute: 30
        timeout_seconds: 4
      context7:
        enabled: true  # No API key required - uses Context7 MCP server
        priority: 4
        max_requests_per_minute: 30
        timeout_seconds: 5
        # Context7-specific configuration for MCP subprocess
        command: "npx"
        args: ["-y", "@upstash/context7-mcp"]
        cache_ttl: 3600
    circuit_breaker:
      failure_threshold: 3
      recovery_timeout: 300
      timeout_seconds: 5
    cache:
      ttl_seconds: 3600
      enabled: true

# API Response Pipeline Configuration
api_response_pipeline:
  cache:
    ttl: 300                # Default cache TTL in seconds
    prefix: "api_response:" # Prefix for cache keys
  default_formatter: "JSONAPIFormatter"  # Default formatter class
  allowed_formatters:
    - "JSONAPIFormatter"
    - "SimpleJSONFormatter"

# Configuration Notes:
# 
# 1. Environment Variables Override:
#    - Use environment variables for sensitive data (API keys, passwords)
#    - Environment variables take highest priority over this file
#    - Example: REDIS_HOST=localhost overrides redis.host above
#
# 2. Database Overrides:
#    - Configuration can be overridden via system_config table
#    - Database overrides have lowest priority
#
# 3. Redis Production Security:
#    - Set REDIS_PASSWORD environment variable for production
#    - Enable SSL/TLS by setting REDIS_SSL=true if needed
#
# 4. API Keys and Tokens:
#    - WEAVIATE_API_KEY: Required for Weaviate vector database integration
#    - GITHUB_API_TOKEN: Required for GitHub repository access
#    - OPENAI_API_KEY: Required if using OpenAI as primary/fallback provider
#    - BRAVE_API_KEY: Required for Brave Search external provider (get free at https://brave.com/search/api/)
#    - GOOGLE_SEARCH_API_KEY: Optional for Google Custom Search API
#    - GOOGLE_SEARCH_ENGINE_ID: Required if using Google Custom Search
#
# 5. Docker Compose Alignment:
#    - Redis memory settings must match docker-compose.yml
#    - Service endpoints assume docker-compose service names
#
# 6. Circuit Breaker Settings:
#    - Configured per service with appropriate timeouts
#    - Prevents cascade failures in distributed architecture
#
# 7. Content Processing Limits:
#    - Prevents processing of excessively large content
#    - Quality threshold filters low-quality content